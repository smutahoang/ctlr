
package larc.ctlr.model;

import larc.ctlr.model.Configure.ModelMode;
import larc.ctlr.tool.*;

import java.io.BufferedWriter;
import java.util.Arrays;
import java.util.Random;
import java.io.File;
import java.io.FileWriter;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

public class MultithreadCTLR {
	private String datapath;
	private String outputpath;
	private static Dataset dataset;
	private static int nTopics;
	private static int batch;

	private static double alpha;// prior for users' interest
	private static double gamma; // variance of topic word distribution

	private static double sigma;// variance of users' authorities
	private static double delta;// variance of users' hubs

	private static double epsilon = 0.0001;
	//private static double lamda = 0.01;
	private static double lamda = 0.2;

	private static boolean initByTopicModeling = true;
	private static boolean onlyLearnAuthorityHub = false;
	private static boolean onlyLearnGibbs = true;

	private static Random rand;

	// Gibbs sampling variables
	// user-topic counts
	private static int[][] n_zu = null; // n_zu[z][u]: number of times topic z
										// is observed in posts by user u
	// topic-word counts
	private static int[][] n_zw = null; // n_wz[z][w]: number of times word w is
	// generated by topic z in a post
	private static int[] sum_nzw = null; // sum_nwz[z]: total number of times
											// words
	// that
	// are generated by topic z in a post

	// topic-word distribution
	private static double[][] topicWordDist = null; // topicWordDist[k][w]: the
	// distribution of word w for topic
	// k. Sum of each words distribution
	// for each k = 1

	private static double[][] optTopicWordDist = null; // optimized
														// topicWordDist[k][w]

	// options for learning
	public static double lineSearch_alpha = 0.0001;
	//public static double lineSearch_alpha = 0.00001;
	public static double lineSearch_beta = 0.1;
	public static int lineSearch_MaxIterations = 12;
	public static double lineSearch_lambda;

	public static int maxIteration_topicalInterest = 10;
	public static int maxIteration_Authorities = 10;
	public static int maxIteration_Hubs = 10;
	public static int max_GibbsEM_Iterations =200;

	//public static int gibbs_BurningPeriods = 10;
	//public static int max_Gibbs_Iterations = 50;
	//public static int gibbs_Sampling_Gap = 2;
	
	public static int gibbs_BurningPeriods = 100;
	public static int max_Gibbs_Iterations = 500;
	public static int gibbs_Sampling_Gap = 20;

	public int nParallelThreads = 10;
	public int[] threadStartIndexes = null;
	public int[] threadEndIndexes = null;

	public static double[] threadLikelihood;

	private double postLastLogLikelidhoodMode0;
	private double postLastLogPerplexityMode0;
	private double postOptLogLikelidhoodMode0;
	private double postOptLogPerplexityMode0;
	private double postLastLogLikelidhoodMode1;
	private double postLastLogPerplexityMode1;
	private double postOptLogLikelidhoodMode1;
	private double postOptLogPerplexityMode1;

	private static ModelMode mode;

	static class ChildThread implements Runnable {
		private int threadId;
		private int threadStartIndex;
		private int threadEndIndex;
		private String runOption;

		public ChildThread(int start, int end, String run) {
			this.threadStartIndex = start;
			this.threadEndIndex = end;
			runOption = run;
		}

		public ChildThread(int _threadId, int start, int end) {
			this.threadId = _threadId;
			this.threadStartIndex = start;
			this.threadEndIndex = end;
			runOption = "getLoglikelihood";
		}

		@Override
		public void run() {
			if (runOption.equals("optTopicInterests")) {
				optTopicalInterests(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("optAuthorities")) {
				optAuthorities(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("optHubs")) {
				optHubs(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("topicSample")) {
				topicSample(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("updateOpt")) {
				updateOptimalParams(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("initPostTopic")) {
				initUser(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("getLoglikelihood")) {
				getLogLikelihood(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("initAuthorityHub")) {
				initAuthorityHub(threadStartIndex, threadEndIndex);
			}
		}

		private void getLogLikelihood(int startIndex, int endIndex) {
			threadLikelihood[threadId] = 0;
			for (int u = startIndex; u < endIndex; u++) {
				threadLikelihood[threadId] += getLikelihood(u);
			}
		}

		private void initUser(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++) {
				initPostTopic(u);
			}
		}

		private void optTopicalInterests(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++)
				altOptimize_topicalInterest(u);
		}

		private void optAuthorities(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++)
				altOptimize_Authorities(u);
		}

		private void optHubs(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++)
				altOptimize_Hubs(u);
		}

		private void topicSample(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++) {
				for (int n = 0; n < dataset.users[u].nPosts; n++) {
					// only consider posts in batch
					if (dataset.users[u].postBatches[n] == batch) {
						if (mode == ModelMode.TWITTER_LDA) {
							samplePostTopic_EMGibbs(u, n);
						} else {
							for (int i = 0; i < dataset.users[u].posts[n].nWords; i++)
								sampleWordTopic_EMGibbs(u, n, i);
						}
					}
				}
			}
		}

		private void updateOptimalParams(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++) {
				User currUser = dataset.users[u];
				for (int z = 0; z < nTopics; z++) {
					currUser.optTopicalInterests[z] = currUser.topicalInterests[z];
					currUser.optAuthorities[z] = currUser.authorities[z];
					currUser.optHubs[z] = currUser.hubs[z];
				}
			}
		}

		private void initAuthorityHub(int startIndex, int endIndex) {
			// NormalDistribution g;
			for (int u = startIndex; u < endIndex; u++) {
				User currUser = dataset.users[u];
				for (int z = 0; z < nTopics; z++) {

					// g = new NormalDistribution(currUser.topicalInterests[z],
					// sigma);
					// currUser.authorities[z] = Math.exp(g.sample());

					// g = new NormalDistribution(currUser.topicalInterests[z],
					// delta);
					// currUser.hubs[z] = Math.exp(g.sample());

					currUser.authorities[z] = Math.exp(currUser.topicalInterests[z]);
					currUser.hubs[z] = Math.exp(currUser.topicalInterests[z]);
				}
			}
		}
	}

	private void getThreadIndexes() {
		threadStartIndexes = new int[nParallelThreads];
		threadEndIndexes = new int[nParallelThreads];
		int chunkLength = Math.floorDiv(dataset.nUsers, nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			threadStartIndexes[i] = i * chunkLength;
			threadEndIndexes[i] = threadStartIndexes[i] + chunkLength;
		}
		threadEndIndexes[nParallelThreads - 1] = dataset.nUsers;
		for (int i = 0; i < nParallelThreads; i++) {
			System.out.printf("thread[%d]: start = %d end = %d\n", i, threadStartIndexes[i], threadEndIndexes[i]);
		}
	}

	/***
	 * 
	 * @param _datasetPath
	 * @param _nTopics
	 */
	public MultithreadCTLR(String _datasetPath, int _nTopics, int _batch, ModelMode _mode, String _outputPath) {
		this.datapath = _datasetPath;
		this.outputpath = _outputPath;
		MultithreadCTLR.nTopics = _nTopics;
		MultithreadCTLR.batch = _batch;
		MultithreadCTLR.mode = _mode;
		MultithreadCTLR.dataset = new Dataset(_datasetPath,MultithreadCTLR.batch,onlyLearnGibbs);
		
	}

	/***
	 * get likelihood of a user
	 * 
	 * @param u
	 * @return
	 */
	private static double getLikelihood(int u) {
		// System.out.printf("computing likelihood with u = %d\n", u);
		// to be written
		// Compute the likelihood to make sure that it is improving L(text) +
		// L(link)
		// value can be more than 1
		// eqn 1 + 4
		double linkLikelihood = 0;
		double linkRelationshipLikelihood = 0;
		double linkAuthorityLikelihood = 0;
		double linkHubLikelihood = 0;
		double postLikelihood = 0;
		double postWordLikelihood = 0;
		double postTopicLikelihood = 0;
		// double postThetaLikelihood = 0;
		// double postTauLikelihood = 0;

		User currUser = dataset.users[u];

		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				int v = currUser.nonFollowers[i];
				User nonFollower = dataset.users[v];

				// Compute H_u * A_v
				double HuAv = 0;
				for (int z = 0; z < nTopics; z++) {
					HuAv += nonFollower.hubs[z] * currUser.authorities[z];
				}
				HuAv = HuAv * lamda;
				double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
				linkRelationshipLikelihood += Math.log(1 - fHuAv);

				if (Double.isInfinite(linkRelationshipLikelihood) || Double.isNaN(linkRelationshipLikelihood)) {
					System.out.printf("[non-Followers] HuAv = %.12f fHuAv = %.12f\n", HuAv, fHuAv);
				}
			}
		}

		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				int v = currUser.followers[i];
				User follower = dataset.users[v];

				// Compute H_u * A_v
				double HuAv = 0;
				for (int z = 0; z < nTopics; z++) {
					HuAv += follower.hubs[z] * currUser.authorities[z];
				}
				HuAv = HuAv * lamda;
				double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
				linkRelationshipLikelihood += Math.log(fHuAv);

				if (Double.isInfinite(linkRelationshipLikelihood) || Double.isNaN(linkRelationshipLikelihood)) {
					System.out.printf("[followers] HuAv = %.12f fHuAv = %.12f\n", HuAv, fHuAv);
				}
			}
		}

		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				// Only compute likelihood of non followings which are in
				// training
				// batch (i.e. batch = 1)
				if (currUser.nonFollowingBatches[i] == batch) {
					int v = currUser.nonFollowings[i];
					User nonFollowing = dataset.users[v];

					// Compute H_u * A_v
					double HuAv = 0;
					for (int z = 0; z < nTopics; z++) {
						HuAv += currUser.hubs[z] * nonFollowing.authorities[z];
					}
					HuAv = HuAv * lamda;
					double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
					linkRelationshipLikelihood += Math.log(1.0 - fHuAv);
					if (Double.isInfinite(linkRelationshipLikelihood) || Double.isNaN(linkRelationshipLikelihood)) {
						System.out.printf("[non-followees] HuAv = %.12f fHuAv = %.12f\n", HuAv, fHuAv);
					}
				}
			}
		}

		// Compute following likelihood
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				// Only compute likelihood of followings which are in
				// training
				// batch
				// (i.e. batch = 1)
				if (currUser.followingBatches[i] == batch) {
					int v = currUser.followings[i];
					User following = dataset.users[v];

					// Compute H_u * A_v
					double HuAv = 0;
					for (int z = 0; z < nTopics; z++) {
						HuAv += currUser.hubs[z] * following.authorities[z];
					}
					HuAv = HuAv * lamda;
					double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
					linkRelationshipLikelihood += Math.log(fHuAv);
					if (Double.isInfinite(linkRelationshipLikelihood) || Double.isNaN(linkRelationshipLikelihood)) {
						System.out.printf("[followees] HuAv = %.12f fHuAv = %.12f\n", HuAv, fHuAv);
					}
				}
			}
		}

		for (int k = 0; k < nTopics; k++) {
			// linkAuthorityLikelihood += (-Math.log(sigma)
			// - (Math.pow(Math.log(currUser.authorities[k]) -
			// currUser.topicalInterests[k], 2)
			// / (2 * Math.pow(sigma, 2))));
			linkAuthorityLikelihood += -Math.pow((Math.log(currUser.authorities[k]) - currUser.topicalInterests[k]), 2)
					/ (2 * Math.pow(delta, 2));

			if (Double.isInfinite(linkAuthorityLikelihood) || Double.isNaN(linkAuthorityLikelihood)) {
				System.out.printf("[authority] A[%d] = %.12f\n", k, currUser.authorities[k]);
			}
			// linkHubLikelihood += (-Math.log(delta)
			// - (Math.pow(Math.log(currUser.hubs[k]) -
			// currUser.topicalInterests[k], 2)
			// / (2 * Math.pow(delta, 2))));

			linkHubLikelihood += -Math.pow((Math.log(currUser.hubs[k]) - currUser.topicalInterests[k]), 2)
					/ (2 * Math.pow(sigma, 2));

			if (Double.isInfinite(linkHubLikelihood) || Double.isNaN(linkHubLikelihood)) {
				System.out.printf("[authority] H[%d] = %.12f\n", k, currUser.hubs[k]);
			}
		}

		linkLikelihood += linkRelationshipLikelihood + linkAuthorityLikelihood + linkHubLikelihood;

		for (int s = 0; s < currUser.nPosts; s++) {
			if (currUser.postBatches[s] == batch) {
				Post currPost = currUser.posts[s];
				if (mode == ModelMode.TWITTER_LDA) {
					for (int w = 0; w < currPost.words.length; w++) {
						int word = currPost.words[w];
						postWordLikelihood += Math.log(topicWordDist[currPost.topic][word]);
					}
					postTopicLikelihood += Math.log(currUser.topicalInterests[currPost.topic]);
					if (Double.isInfinite(postTopicLikelihood) || Double.isNaN(postTopicLikelihood)) {
						System.out.printf("[Post] Theta[%d] = %.12f\n", currPost.topic,
								currUser.topicalInterests[currPost.topic]);
					}
				} else {
					for (int i = 0; i < currPost.nWords; i++) {
						int word = currPost.words[i];
						int topic = currPost.wordTopics[i];
						postWordLikelihood += Math.log(topicWordDist[topic][word]);
						postTopicLikelihood += Math.log(currUser.topicalInterests[topic]);
						if (Double.isInfinite(postTopicLikelihood) || Double.isNaN(postTopicLikelihood)) {
							System.out.printf("[Post] Theta[%d] = %.12f\n", currPost.topic,
									currUser.topicalInterests[currPost.topic]);
						}
					}

				}
			}
		}
		postLikelihood = postWordLikelihood + postTopicLikelihood;
		// for (int k = 0; k < nTopics; k++) {
		// postThetaLikelihood += (alpha - 1) *
		// Math.log(currUser.topicalInterests[k]);
		// }
		if (Double.isInfinite(linkLikelihood) || Double.isNaN(linkLikelihood) || Double.isInfinite(postLikelihood)
				|| Double.isNaN(postLikelihood)) {
			System.out.println("In getLikelihood(int u): infinite!");
			System.exit(-1);
		}
		return (linkLikelihood + postLikelihood);
	}

	/***
	 * get likelihood of the whole dataset
	 * 
	 * @return
	 */
	private double getLikelihood() {
		double loglikelihood = 0;
		ExecutorService executor = Executors.newFixedThreadPool(nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			Runnable worker = new ChildThread(i, threadStartIndexes[i], threadEndIndexes[i]);
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
			// do nothing, just wait for the threads to finish
		}
		for (int i = 0; i < nParallelThreads; i++) {
			loglikelihood += threadLikelihood[i];
		}
		return loglikelihood;
	}

	/***
	 * compute likelihood of data as a function of topical interest of u when
	 * the interest is x, i.e., if L(data|parameters) = f(theta_u) +
	 * const-of-theta_u then this function returns f(x)
	 * 
	 * @param u
	 * @return
	 */
	private static double getLikelihood_topicalInterest(int u, double[] x) {
		// Refer to Eqn 9 in Learning paper for Formula

		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double postLikelihood = 0;
		double topicLikelihood = 0;
		double finalLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		for (int k = 0; k < nTopics; k++) {
			authorityLikelihood += -Math.pow((Math.log(currUser.authorities[k]) - x[k]), 2) / (2 * Math.pow(sigma, 2));
		}

		for (int k = 0; k < nTopics; k++) {
			hubLikelihood += -Math.pow((Math.log(currUser.hubs[k]) - x[k]), 2) / (2 * Math.pow(delta, 2));
		}

		for (int i = 0; i < currUser.nPosts; i++) {
			// Only compute post likelihood of posts which are in batch (i.e.
			// training batch = 1)
			if (currUser.postBatches[i] == batch) {
				if (mode == ModelMode.TWITTER_LDA) {
					int postTopic = currUser.posts[i].topic;
					// postLikelihood += x[postTopic];
					postLikelihood += Math.log(x[postTopic]);
				} else {
					Post currPost = currUser.posts[i];
					for (int j = 0; j < currPost.nWords; j++) {
						int wordTopic = currPost.wordTopics[j];
						postLikelihood += Math.log(x[wordTopic]);
					}
				}
			}
		}

		//for (int k = 0; k < nTopics; k++) {
		//	topicLikelihood += (alpha - 1) * Math.log(x[k]);
		//}
		// finalLikelihood = authorityLikelihood + hubLikelihood +
		// postLikelihood + topicLikelihood;

		/*
		 * System.out.printf(
		 * "[TopicLikelihood] u = %d authorityLikelihood = %f hubLikelihood = %f postLikelihood = %f\n"
		 * , u, authorityLikelihood, hubLikelihood, postLikelihood);
		 */

		finalLikelihood = authorityLikelihood + hubLikelihood + postLikelihood + topicLikelihood;
		// finalLikelihood = authorityLikelihood + hubLikelihood +
		// postLikelihood;
		//System.out.println(finalLikelihood);
		return finalLikelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to interest of u in
	 * topic k when the interest is x, i.e., if if L(data|parameters) =
	 * f(theta_u) + const-of-theta_u then this function return df/dtheta_uk at
	 * theta_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private static double gradLikelihood_topicalInterest(int u, int k, double x) {
		// Refer to Eqn 11 in Learning paper

		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double postLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		authorityLikelihood = ((Math.log(currUser.authorities[k]) - x) / Math.pow(delta, 2));

		hubLikelihood = ((Math.log(currUser.hubs[k]) - x) / Math.pow(sigma, 2));

		for (int i = 0; i < currUser.nPosts; i++) {
			// Only compute post likelihood of posts which are in batch
			// (i.e. training batch = 1)
			if (currUser.postBatches[i] == batch) {
				// Only consider posts which are assigned topic k (i.e. z_{v,s}
				// = k)
				if (mode == ModelMode.TWITTER_LDA) {
					if (currUser.posts[i].topic == k) {
						postLikelihood += 1 / x;
					}
				} else {
					Post currPost = currUser.posts[i];
					for (int j = 0; j < currPost.nWords; j++) {
						if (currPost.wordTopics[j] == k) {
							postLikelihood += 1 / x;
						}
					}
				}
			}

		}
		gradLikelihood = authorityLikelihood + hubLikelihood + postLikelihood;
		//gradLikelihood = authorityLikelihood + hubLikelihood + postLikelihood + ((alpha - 1) / x);
		// gradLikelihood = authorityLikelihood + hubLikelihood +
		// postLikelihood;

		/*
		 * System.out.printf(
		 * "[Grad-TopicLikelihood] u = %d authorityLikelihood = %f hubLikelihood = %f postLikelihood = %f\n"
		 * , u, authorityLikelihood, hubLikelihood, postLikelihood);
		 */

		return gradLikelihood;
	}

	/***
	 * get projection of x on n-dimension simplex i.e., find the n-dimension
	 * probability distribution closest to x
	 * 
	 * @param x
	 * @param n
	 * @return
	 */
	private static double[] simplexProjection(double[] x, double z) {
		// given all the k that u have, it adds up to 1
		// Refer to https://github.com/blei-lab/ctr/blob/master/opt.cpp
		double[] projX = new double[x.length];

		// copy the content of x into projX
		for (int i = 0; i < x.length; i++) {
			projX[i] = x[i];
		}
		// Sort projX in asc order
		Arrays.sort(projX);

		// Compute the sum of the offset
		double cumsum = -z;
		double p = 0;
		int j = 0;
		for (int i = x.length - 1; i >= 0; i--) {
			p = x[i];
			cumsum += p;
			if (p > cumsum / (j + 1)) {
				j++;
			} else {
				break;
			}
		}

		// Compute the offset for each topic
		double theta = cumsum / j;
		for (int i = 0; i < x.length; i++) {
			p = x[i] - theta;
			if (p < 0) {
				//p = 0.0;
				p = epsilon;
			}
			projX[i] = p;
		}

		double sum_x = 0;
		for (int i = 0; i < projX.length; i++) {
			sum_x += projX[i];
		}

		for (int i = 0; i < projX.length; i++) {
			projX[i] = projX[i] / sum_x;
		}

		return projX;
	}

	/***
	 * alternating step to optimize topical interest of u
	 * 
	 * @param u
	 */
	static void altOptimize_topicalInterest(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].topicalInterests;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_topicalInterest(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		// parameters for line search
		// lineSearch_alpha = 0.0001;
		// lineSearch_beta = 0.1;
		// lineSearch_MaxIterations = 5;

		for (int iter = 0; iter < maxIteration_topicalInterest; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_topicalInterest(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
				}

				x = simplexProjection(x, 1);// this step to make sure that we

				// compute f at the new x
				f = 0 - getLikelihood_topicalInterest(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_topic: u = %d iter = %d f = %f\n", u,
				// iter, f);
			} else {
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_topic: u = %d iter = %d f = %f\n", u,
				// iter, f);
				break;// cannot improve further
			}
		}
	}

	/***
	 * compute likelihood of data as a function of authority of u when the
	 * authority is x, i.e., if L(data|parameters) = f(A_u) + const-of-A_u then
	 * this function returns f(x)
	 * 
	 * @param v
	 * @param x[]
	 * @return
	 */
	private static double getLikelihood_authority(int v, double[] x) {
		// Refer to Eqn 13 in Learning paper

		double followerLikelihood = 0;
		double nonFollowerLikelihood = 0;
		double postLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[v];

		// Compute non follower likelihood
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				int u = currUser.nonFollowers[i];
				User nonFollower = dataset.users[u];

				// Compute H_u * A_v
				double HuAv = 0;
				for (int z = 0; z < nTopics; z++) {
					HuAv += nonFollower.hubs[z] * x[z];// now A_v is x
				}
				HuAv = HuAv * lamda;
				// System.out.println("HuAv:"+HuAv);
				double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
				// System.out.println("fHuAv:"+fHuAv);
				nonFollowerLikelihood += Math.log(1 - fHuAv);
			}
		}

		// Compute follower likelihood
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				int u = currUser.followers[i];
				User follower = dataset.users[u];

				// Compute H_u * A_v
				double HuAv = 0;
				for (int z = 0; z < nTopics; z++) {
					HuAv += follower.hubs[z] * x[z];// now A_v is x
				}
				HuAv = HuAv * lamda;
				double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
				followerLikelihood += Math.log(fHuAv);
			}
		}

		// Compute post likelihood
		for (int k = 0; k < nTopics; k++) {
			postLikelihood += Math.pow((Math.log(x[k]) - currUser.topicalInterests[k]), 2) / (2 * Math.pow(sigma, 2));
		}

		likelihood = nonFollowerLikelihood + followerLikelihood - postLikelihood;
		// System.out.println("Num of Non Followers:" +
		// currUser.nonFollowers.length);
		// System.out.println("NonFollowersLikelihood:" +
		// nonFollowerLikelihood);
		// System.out.println("Num of Followers:" + currUser.followers.length);
		// System.out.println("FollowerLikelihood:" + followerLikelihood);
		// System.out.println("PostLikelihood:" + postLikelihood);

		return likelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to authority of u in
	 * topic k when the authority is x, i.e., if if L(data|parameters) = f(A_u)
	 * + const-of-A_u then this function return df/dA_uk at A_uk = x
	 * 
	 * @param v
	 * @param k
	 * @param x
	 * @return
	 */
	private static double gradLikelihood_authority(int v, int k, double x) {
		// Refer to Eqn 15 in Learning paper

		double followerLikelihood = 0;
		double nonFollowerLikelihood = 0;
		double postLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[v];

		// Compute non follower likelihood
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				int u = currUser.nonFollowers[i];
				User nonFollower = dataset.users[u];

				// Compute H_u * A_v
				double HuAv = 0;
				for (int z = 0; z < nTopics; z++) {
					if (z == k) {
						HuAv += nonFollower.hubs[z] * x;
					} else {
						HuAv += nonFollower.hubs[z] * currUser.authorities[z];
					}
				}
				HuAv = HuAv * lamda;
				nonFollowerLikelihood += -(lamda * nonFollower.hubs[k])
						- 1 / (Math.exp(-HuAv) + 1) * Math.exp(-HuAv) * -(lamda * nonFollower.hubs[k]);
			}
		}

		// Compute follower likelihood
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				int u = currUser.followers[i];
				User follower = dataset.users[u];

				// Compute H_u * A_v
				double HuAv = 0;
				for (int z = 0; z < nTopics; z++) {
					if (z == k) {
						HuAv += follower.hubs[z] * x;
					} else {
						HuAv += follower.hubs[z] * currUser.authorities[z];
					}
				}
				HuAv = HuAv * lamda;
				followerLikelihood += ((1 / (1 - Math.exp(-HuAv))) * -Math.exp(-HuAv) * -(lamda * follower.hubs[k]))
						- ((1 / (Math.exp(-HuAv) + 1)) * Math.exp(-HuAv) * -(lamda * follower.hubs[k]));

			}
		}

		postLikelihood = ((Math.log(x) - currUser.topicalInterests[k]) / Math.pow(sigma, 2)) * (1 / x);

		gradLikelihood = nonFollowerLikelihood + followerLikelihood - postLikelihood;

		return gradLikelihood;
	}

	/***
	 * alternating step to optimize authorities of user u
	 * 
	 * @param u
	 */

	static void altOptimize_Authorities(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].authorities;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_authority(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		// parameters for line search
		// lineSearch_alpha = 0.0001;
		// lineSearch_beta = 0.1;
		// lineSearch_MaxIterations = 5;

		for (int iter = 0; iter < maxIteration_Authorities; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_authority(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
					if (x[k] < epsilon) {
						x[k] = epsilon;
					}
				}

				// compute f at the new x
				f = 0 - getLikelihood_authority(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
					// if (dataset.users[u].userId.equals("409147008")){
					// System.out.println("k:"+k+" x:"+ currentX[k]);;
					// }
				}
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_authority: u = %d iter = %d f = %f\n",
				// u, iter, f);
			} else {
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_authority: u = %d iter = %d f = %f\n",
				// u, iter, f);
				break;// cannot improve further
			}
		}
	}

	/***
	 * compute likelihood of data as a function of hub of u when the hub is x,
	 * i.e., if L(data|parameters) = f(H_u) + const-of-H_u then this function
	 * returns f(x)
	 * 
	 * @param u
	 * @param x[]
	 * @return
	 */
	private static double getLikelihood_hub(int u, double[] x) {
		// Refer to Eqn 17 in Learning paper

		double followingLikelihood = 0;
		double nonFollowingLikelihood = 0;
		double postLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		// Compute non following likelihood
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				// Only compute likelihood of non followings which are in
				// training batch (i.e. batch = 1)
				if (currUser.nonFollowingBatches[i] == batch) {
					int v = currUser.nonFollowings[i];
					User nonFollowing = dataset.users[v];

					// Compute H_u * A_v
					double HuAv = 0;
					for (int z = 0; z < nTopics; z++) {
						HuAv += x[z] * nonFollowing.authorities[z];
					}
					HuAv = HuAv * lamda;
					double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
					nonFollowingLikelihood += Math.log(1.0 - fHuAv);
				}
			}
		}

		// Compute following likelihood
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				// Only compute likelihood of followings which are in training
				// batch
				// (i.e. batch = 1)
				if (currUser.followingBatches[i] == batch) {
					int v = currUser.followings[i];
					User following = dataset.users[v];
					// Compute H_u * A_v
					double HuAv = 0;
					for (int z = 0; z < nTopics; z++) {
						HuAv += x[z] * following.authorities[z];
					}
					HuAv = HuAv * lamda;
					double fHuAv = 2 * ((1 / (Math.exp(-HuAv) + 1)) - 0.5);
					followingLikelihood += Math.log(fHuAv);
				}
			}
		}

		// Compute post likelihood
		for (int k = 0; k < nTopics; k++) {
			postLikelihood += Math.pow(Math.log(x[k]) - currUser.topicalInterests[k], 2) / (2 * Math.pow(delta, 2));
		}

		likelihood = nonFollowingLikelihood + followingLikelihood - postLikelihood;

		return likelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to hub of u in topic
	 * k when the hub is x, i.e., if if L(data|parameters) = f(H_u) +
	 * const-of-H_u then this function return df/dH_uk at H_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private static double gradLikelihood_hub(int u, int k, double x) {
		// Refer to Eqn 19 in Learning paper

		double followingLikelihood = 0;
		double nonFollowingLikelihood = 0;
		double postLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		// Compute non following likelihood
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				// Only compute likelihood of non followings which are in
				// training
				// batch (i.e. batch = 1)
				if (currUser.nonFollowingBatches[i] == batch) {
					int v = currUser.nonFollowings[i];
					User nonFollowing = dataset.users[v];

					// Compute H_u * A_v
					double HuAv = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HuAv += x * nonFollowing.authorities[z];
						} else {
							HuAv += currUser.hubs[z] * nonFollowing.authorities[z];
						}
					}
					HuAv = HuAv * lamda;
					nonFollowingLikelihood += -(lamda * nonFollowing.authorities[k])
							- 1 / (Math.exp(-HuAv) + 1) * Math.exp(-HuAv) * -(lamda * nonFollowing.authorities[k]);
				}
			}
		}

		// Compute following likelihood
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				// Only compute likelihood of followings which are in training
				// batch
				// (i.e. batch = 1)
				if (currUser.followingBatches[i] == batch) {
					int v = currUser.followings[i];
					User following = dataset.users[v];

					// Compute H_u * A_v
					double HuAv = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HuAv += x * following.authorities[z];
						} else {
							HuAv += currUser.hubs[z] * following.authorities[z];
						}
					}
					HuAv = HuAv * lamda;
					followingLikelihood += ((1 / (1 - Math.exp(-HuAv))) * -Math.exp(-HuAv)
							* -(lamda * following.authorities[k]))
							- ((1 / (Math.exp(-HuAv) + 1)) * Math.exp(-HuAv) * -(lamda * following.authorities[k]));
				}
			}
		}

		postLikelihood = ((Math.log(x) - currUser.topicalInterests[k]) / Math.pow(delta, 2)) * (1 / x);

		gradLikelihood = nonFollowingLikelihood + followingLikelihood - postLikelihood;

		return gradLikelihood;
	}

	/***
	 * alternating step to optimize hubs of user u
	 * 
	 * @param u
	 */
	static void altOptimize_Hubs(int u) {
		// the following code is just a draft, to be corrected.
		// will need more checking, but roughly the gradient descent
		// for learning A_u consists of the following main steps
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].hubs;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_hub(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		// parameters for line search
		// lineSearch_alpha = 0.0001;
		// lineSearch_beta = 0.1;
		// lineSearch_MaxIterations = 5;

		for (int iter = 0; iter < maxIteration_Hubs; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_hub(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
					if (x[k] < epsilon) {
						x[k] = epsilon;
					}
				}

				// compute f at the new x
				f = 0 - getLikelihood_hub(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u,
				// iter, f);
			} else {
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u,
				// iter, f);
				break;// cannot improve further
			}
		}
	}

	/***
	 * alternating step to optimize topics' word distribution
	 */
	private void altOptimize_topics() {
		// initialize the count variables
		for (int k = 0; k < nTopics; k++) {
			sum_nzw[k] = 0;
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				n_zw[k][w] = 0;
			}
		}

		// update count variable base on the post topic assigned
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int n = 0; n < currUser.posts.length; n++) {
				Post currPost = currUser.posts[n];
				// only consider posts in batch
				if (currUser.postBatches[n] == batch) {
					if (mode == ModelMode.TWITTER_LDA) {
						int z = currPost.topic;
						for (int i = 0; i < currPost.nWords; i++) {
							int wordIndex = currPost.words[i];
							sum_nzw[z] += 1;
							n_zw[z][wordIndex] += 1;
						}
					} else {
						for (int i = 0; i < currPost.nWords; i++) {
							int wordIndex = currPost.words[i];
							int z = currPost.wordTopics[i];
							sum_nzw[z] += 1;
							n_zw[z][wordIndex] += 1;
						}
					}
				}
			}
		}

		// compute topic word distribution
		for (int k = 0; k < nTopics; k++) {
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				topicWordDist[k][w] = (n_zw[k][w] + gamma) / (sum_nzw[k] + gamma * dataset.vocabulary.length);
			}
		}
	}

	/***
	 * to sample topic for post n of user u
	 * 
	 * @param u
	 * @param n
	 */

	private static void samplePostTopic_EMGibbs(int u, int n) {
		// Set the current user to be u
		User currUser = dataset.users[u];

		double sump = 0;
		// p: p(z_u,s = z| rest)

		double[] p = new double[nTopics];
		double max = -Double.MAX_VALUE;
		for (int z = 0; z < nTopics; z++) {
			// User-topic
			p[z] = Math.log(currUser.topicalInterests[z]);

			// topic-word
			Post currPost = currUser.posts[n];
			for (int w = 0; w < currPost.words.length; w++) {
				int word = currPost.words[w];
				p[z] += Math.log(topicWordDist[z][word]);
			}

			// update min
			if (max < p[z]) {
				max = p[z];
			}

		}
		double[] plog = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			plog[z] = p[z];
		}
		// convert log(sump) to probability
		for (int z = 0; z < nTopics; z++) {
			p[z] = p[z] - max;
			p[z] = Math.exp(p[z]);

			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}

		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z]) {
				continue;
			} else {
				// Sample topic
				currUser.posts[n].topic = z;
				/*
				 * if (currUser.topicalInterests[z] <= 10E-12) {
				 * System.err.println("Something wrong!!! "); for (int k = 0; k
				 * < nTopics; k++) { System.out.printf(
				 * "theta[%d] = %.12f \tlog(theta[%d]) = %.12f \tplog[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n"
				 * , k, currUser.topicalInterests[k], k,
				 * Math.log(currUser.topicalInterests[k]), k, plog[k], k, p[k],
				 * sump); } System.out.printf("z = %d\n", z); System.exit(-1); }
				 */
				return;
			}
		}
		System.err.println("Something wrong!!! ");
		for (int k = 0; k < nTopics; k++) {
			System.out.printf("theta[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n", k, currUser.topicalInterests[k], k,
					p[k], sump);
		}
		System.exit(-1);
	}

	private void samplePostTopic_Gibbs(int u, int n) {
		Post currPost = dataset.users[u].posts[n];
		// reduce current counts
		int currZ = currPost.topic;
		n_zu[currZ][u]--;
		if (n_zu[currZ][u] < 0) {
			System.out.printf("u = %d z = %d n_zu = %d\n", u, currZ, n_zu[currZ][u]);
			System.exit(-1);
		}
		sum_nzw[currZ] -= currPost.nWords;
		for (int w = 0; w < currPost.nWords; w++) {
			int word = currPost.words[w];
			n_zw[currZ][word]--;
		}

		double sump = 0;
		// p: p(z_u,s = z| rest)

		double[] p = new double[nTopics];
		double max = -Double.MAX_VALUE;
		for (int z = 0; z < nTopics; z++) {
			// User-topic
			p[z] = Math.log(n_zu[z][u] + alpha);
			// topic-word
			for (int w = 0; w < currPost.nWords; w++) {
				int word = currPost.words[w];
				p[z] += Math.log((n_zw[z][word] + gamma) / (sum_nzw[z] + gamma * dataset.vocabulary.length));
			}
			// update min
			if (max < p[z]) {
				max = p[z];
			}

		}
		// convert log(sump) to probability
		for (int z = 0; z < nTopics; z++) {
			p[z] = p[z] - max;
			p[z] = Math.exp(p[z]);
			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}

		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z]) {
				continue;
			} else {
				// Sample topic
				currPost.topic = z;
				// increase the counts
				n_zu[z][u]++;
				sum_nzw[z] += currPost.nWords;
				for (int w = 0; w < currPost.nWords; w++) {
					int word = currPost.words[w];
					n_zw[z][word]++;
				}
				return;
			}
		}
		System.err.println("Something wrong!!! ");
		for (int k = 0; k < nTopics; k++) {
			System.out.printf("theta[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n", k,
					dataset.users[u].topicalInterests[k], k, p[k], sump);
		}
		System.exit(-1);
	}

	private static void sampleWordTopic_EMGibbs(int u, int n, int i) {
		// Set the current user to be u
		User currUser = dataset.users[u];
		Post currPost = currUser.posts[n];
		int word = currPost.words[i];

		double sump = 0;
		// p: p(z_u,s = z| rest)

		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			// User-topic
			p[z] = currUser.topicalInterests[z];
			// topic-word
			p[z] *= topicWordDist[z][word];
			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}

		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z]) {
				continue;
			} else {
				// Sample topic
				currUser.posts[n].wordTopics[i] = z;
				/*
				 * if (currUser.topicalInterests[z] <= 10E-12) {
				 * System.err.println("Something wrong!!! "); for (int k = 0; k
				 * < nTopics; k++) { System.out.printf(
				 * "p[%d] = %.12f sump = %.12f\n", k, p[k], sump);
				 * System.out.printf("z = %d", z); } System.exit(-1); }
				 */
				return;
			}
		}
		System.err.println("Something wrong 1352!!! ");
		for (int k = 0; k < nTopics; k++) {
			System.out.printf("theta[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n", k, currUser.topicalInterests[k], k,
					p[k], sump);
		}
		System.exit(-1);
	}

	private void sampleWordTopic_Gibbs(int u, int n, int i) {
		Post currPost = dataset.users[u].posts[n];
		// reduce the counts
		int currZ = currPost.wordTopics[i];
		int word = currPost.words[i];

		n_zu[currZ][u]--;
		n_zw[currZ][word]--;
		sum_nzw[currZ]--;

		double sump = 0;
		// p: p(z_u,s = z| rest)

		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			// User-topic
			p[z] = n_zu[z][u] + alpha;
			// topic-word
			p[z] *= (n_zw[z][word] + gamma) / (sum_nzw[z] + gamma * dataset.vocabulary.length);
			p[z] = sump + p[z];
			sump = p[z];
		}

		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z]) {
				continue;
			} else {
				// Sample topic
				currPost.wordTopics[i] = z;

				// increase the counts
				n_zu[z][u]++;
				n_zw[z][word]++;
				sum_nzw[z]++;
				/*
				 * if (currUser.topicalInterests[z] <= 10E-12) {
				 * System.err.println("Something wrong!!! "); for (int k = 0; k
				 * < nTopics; k++) { System.out.printf(
				 * "p[%d] = %.12f sump = %.12f\n", k, p[k], sump);
				 * System.out.printf("z = %d", z); } System.exit(-1); }
				 */
				return;
			}
		}
		System.err.println("Something wrong 1405!!! ");
		for (int k = 0; k < nTopics; k++) {
			System.out.printf("theta[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n", k,
					dataset.users[u].topicalInterests[k], k, p[k], sump);
		}
		System.exit(-1);
	}

	/****
	 * initialize topic assignment for posts of user u
	 * 
	 * @param u
	 */
	private static void initPostTopic(int u) {
		// System.out.printf("initializing for user %d\n", u);
		User currUser = dataset.users[u];
		for (int n = 0; n < currUser.posts.length; n++) {
			// only consider posts in batch
			if (currUser.postBatches[n] == batch) {
				if (mode == ModelMode.TWITTER_LDA) {
					int randTopic = rand.nextInt(nTopics);
					currUser.posts[n].topic = randTopic;
				} else {
					Post currPost = currUser.posts[n];
					for (int i = 0; i < currPost.nWords; i++) {
						int randTopic = rand.nextInt(nTopics);
						currPost.wordTopics[i] = randTopic;
					}
				}
			}
		}
	}

	/***
	 * initilize users' topical interest and topics' word distribution by
	 * TwitterLDA/OriginalLDA
	 */
	private void gibbsInit() {
		// initialize the count variables
		int[][] final_n_zw = new int[nTopics][dataset.vocabulary.length];
		int[] final_sum_nzw = new int[nTopics];
		int[][] final_n_zu = new int[nTopics][dataset.nUsers];

		for (int z = 0; z < nTopics; z++) {
			sum_nzw[z] = 0;
			final_sum_nzw[z] = 0;
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				n_zw[z][w] = 0;
				final_n_zw[z][w] = 0;
			}
			for (int u = 0; u < dataset.nUsers; u++) {
				n_zu[z][u] = 0;
				final_n_zu[z][u] = 0;
			}
		}
		// update count variable base on the posts' topic assignment
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int n = 0; n < currUser.posts.length; n++) {
				Post currPost = currUser.posts[n];
				// only consider posts in batch
				if (currUser.postBatches[n] == batch) {
					if (mode == ModelMode.TWITTER_LDA) {
						int z = currPost.topic;
						n_zu[z][u]++;
						sum_nzw[z] += currPost.nWords;
						for (int w = 0; w < currPost.nWords; w++) {
							int wordIndex = currPost.words[w];
							n_zw[z][wordIndex]++;
						}
					} else {
						for (int i = 0; i < currPost.nWords; i++) {
							int wordIndex = currPost.words[i];
							int z = currPost.wordTopics[i];
							n_zu[z][u]++;
							sum_nzw[z]++;
							n_zw[z][wordIndex]++;
						}
					}
				}
			}
		}
		// gibss sampling
		for (int iter = 0; iter < gibbs_BurningPeriods + max_Gibbs_Iterations; iter++) {
			System.out.println("Gibb Iteration:" + iter);
			for (int u = 0; u < dataset.nUsers; u++) {
				for (int n = 0; n < dataset.users[u].nPosts; n++) {
					if (dataset.users[u].postBatches[n] == batch) {
						if (mode == ModelMode.TWITTER_LDA) {
							samplePostTopic_Gibbs(u, n);
						} else {
							for (int i = 0; i < dataset.users[u].posts[n].nWords; i++) {
								sampleWordTopic_Gibbs(u, n, i);
							}
						}
					}
				}
			}

			if (iter < gibbs_BurningPeriods) {
				continue;
			}
			if (iter % gibbs_Sampling_Gap != 0) {
				continue;
			}

			for (int z = 0; z < nTopics; z++) {
				final_sum_nzw[z] += sum_nzw[z];
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					final_n_zw[z][w] += n_zw[z][w];
				}
				for (int u = 0; u < dataset.nUsers; u++) {
					final_n_zu[z][u] += n_zu[z][u];
				}
			}
		}

		int[] final_sum_nzu = new int[dataset.nUsers];
		for (int u = 0; u < dataset.nUsers; u++) {
			final_sum_nzu[u] = 0;
		}

		// topics
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				topicWordDist[z][w] = (final_n_zw[z][w] + gamma)
						/ (final_sum_nzw[z] + gamma * dataset.vocabulary.length);
			}
			for (int u = 0; u < dataset.nUsers; u++) {
				final_sum_nzu[u] += final_n_zu[z][u];
			}
		}
		// users' topical interests
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int z = 0; z < nTopics; z++) {
				dataset.users[u].topicalInterests[z] = (final_n_zu[z][u] + alpha)
						/ (final_sum_nzu[u] + nTopics * alpha);

				if (dataset.users[u].topicalInterests[z] < 0) {
					System.out.printf("u = %d z = %d theta = %f\n", u, z, dataset.users[u].topicalInterests[z]);
					System.exit(-1);
				}
			}
		}
	}

	/***
	 * initialize the data before training
	 */
	private void init() {
		alpha = (double) (20) / (double) (nTopics);// prior for users' interest
		gamma = 0.001;
		sigma = 0.2;// variance of users' authorities
		delta = 0.2;// variance of users' hubs
		rand = new Random();

		// allocate memory for counts
		n_zu = new int[nTopics][dataset.nUsers];
		n_zw = new int[nTopics][dataset.vocabulary.length];
		sum_nzw = new int[nTopics];
		// allocate memory for the users
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			currUser.authorities = new double[nTopics];
			currUser.hubs = new double[nTopics];
			currUser.topicalInterests = new double[nTopics];
			currUser.optAuthorities = new double[nTopics];
			currUser.optHubs = new double[nTopics];
			currUser.optTopicalInterests = new double[nTopics];
			if (mode == ModelMode.ORIGINAL_LDA) {
				for (int n = 0; n < dataset.users[u].nPosts; n++) {
					dataset.users[u].posts[n].wordTopics = new int[dataset.users[u].posts[n].nWords];
				}
			}
		}

		// init topic assignment for posts
		ExecutorService executor = Executors.newFixedThreadPool(nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "initPostTopic");
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
			// do nothing, just wait for the threads to finish
		}

		// allocate memory for topics
		topicWordDist = new double[nTopics][dataset.vocabulary.length];
		optTopicWordDist = new double[nTopics][dataset.vocabulary.length];

		// init topics' word topical interest
		if (initByTopicModeling) {
			// initialize by topic modeling
			gibbsInit();
		} else {
			// topics
			altOptimize_topics();
			// users' topical interest
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				int nUWords = 0;
				for (int z = 0; z < nTopics; z++) {
					n_zu[z][u] = 0;
				}
				for (int n = 0; n < currUser.posts.length; n++) {
					// only consider posts in batch
					if (currUser.postBatches[n] == batch) {
						if (mode == ModelMode.TWITTER_LDA) {
							n_zu[currUser.posts[n].topic][u] += 1;
						} else {
							Post currPost = currUser.posts[n];
							for (int i = 0; i < currPost.nWords; i++) {
								n_zu[currPost.wordTopics[i]][u] += 1;
							}
							nUWords += currPost.nWords;
						}
					}
				}
				for (int z = 0; z < nTopics; z++) {
					if (mode == ModelMode.TWITTER_LDA) {
						currUser.topicalInterests[z] = (n_zu[z][u] + alpha) / (currUser.nPosts + nTopics * alpha);
					} else {
						currUser.topicalInterests[z] = (n_zu[z][u] + alpha) / (nUWords + nTopics * alpha);
					}
				}
			}

		}
		// init authority & hub
		executor = Executors.newFixedThreadPool(nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "initAuthorityHub");
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
			// do nothing, just wait for the threads to finish
		}
	}

	/***
	 * modeling learning
	 */
	public void train() {
		System.out.println("initializing");
		getThreadIndexes();
		init();
		
		if (onlyLearnGibbs){
			// TopicWordsDist
			for (int z = 0; z < nTopics; z++) {
				for (int w = 0; w < dataset.vocabulary.length; w++)
					optTopicWordDist[z][w] = topicWordDist[z][w];
			}
			
			// UserTopicalInterest
			for (int u=0; u<dataset.nUsers; u++){
				for (int z = 0; z < nTopics; z++) {
					dataset.users[u].optTopicalInterests[z] = dataset.users[u].topicalInterests[z];
				}
			}
			getOptLikelihoodPerplexity_TwitterLDA();
			getOptLikelihoodPerplexity_OriginalLDA();
			getLastLikelihoodPerplexity_TwitterLDA();
			getLastLikelihoodPerplexity_OriginalLDA();
			output_GibbTopicInterest();
			output_OptPostTopicTopWords(50);
			//output_OptTopicWord();
			output_OptLikelihoodPerplexityMode0();
			output_OptLikelihoodPerplexityMode1();
			return;
		}

		System.out.println("TEST");
		for (int z = 0; z < nTopics; z++) {
			System.out.printf("theta[0][%d] = %f\n", z, dataset.users[0].topicalInterests[z]);
		}

		ExecutorService executor = null;
		double maxLikelihood = -Double.MAX_VALUE;
		double currentLikelihood = 0;
		System.out.println("learning with:");
		System.out.println("\tDatapath:" + this.datapath);
		System.out.println("\tAlpha:" + MultithreadCTLR.alpha);
		System.out.println("\tLine Search Alpha:" + MultithreadCTLR.lineSearch_alpha);
		System.out.println("\tLine Search Beta:" + MultithreadCTLR.lineSearch_beta);
		System.out.println("\tLine Search Max Iterations:" + MultithreadCTLR.lineSearch_MaxIterations);
		System.out.println("\t#Topics:" + MultithreadCTLR.nTopics);
		
		threadLikelihood = new double[nParallelThreads];
		for (int iter = 0; iter < max_GibbsEM_Iterations; iter++) {
			// EM part that employs alternating optimization
			System.out.printf("[iter-%d] optimizing users' topical interest\n", iter);
			executor = Executors.newFixedThreadPool(nParallelThreads);
			for (int i = 0; i < nParallelThreads; i++) {
				Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "optTopicInterests");
				executor.execute(worker);
			}
			executor.shutdown();
			while (!executor.isTerminated()) {
				// do nothing, just wait for the threads to finish
			}

			System.out.printf("[iter-%d] optimizing users' authorities\n", iter);
			executor = Executors.newFixedThreadPool(nParallelThreads);
			for (int i = 0; i < nParallelThreads; i++) {
				Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "optAuthorities");
				executor.execute(worker);
			}
			executor.shutdown();
			while (!executor.isTerminated()) {
				// do nothing, just wait for the threads to finish
			}

			System.out.printf("[iter-%d] optimizing users' hubs\n", iter);
			executor = Executors.newFixedThreadPool(nParallelThreads);
			for (int i = 0; i < nParallelThreads; i++) {
				Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "optHubs");
				executor.execute(worker);
			}
			executor.shutdown();
			while (!executor.isTerminated()) {
				// do nothing, just wait for the threads to finish
			}

			if (!onlyLearnAuthorityHub) {
				System.out.printf("[iter-%d] optimizing topics' word distribution\n", iter);
				altOptimize_topics();

				// Gibbs part
				System.out.printf("[iter-%d] sampling topic for users' posts\n", iter);
				executor = Executors.newFixedThreadPool(nParallelThreads);
				for (int i = 0; i < nParallelThreads; i++) {
					Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "topicSample");
					executor.execute(worker);
				}
				executor.shutdown();
				while (!executor.isTerminated()) {
					// do nothing, just wait for the threads to finish
				}
			}

			// set first Likelihood as the maxLikelihood
			System.out.printf("[iter-%d] computing new likelihood\n", iter);
			currentLikelihood = getLikelihood();

			if (maxLikelihood < currentLikelihood) {
				System.out.printf("[iter-%d] saving solution\n", iter);
				maxLikelihood = currentLikelihood;
				// set optimized topicWordDist to be the current
				// TopicWordsDist
				for (int z = 0; z < nTopics; z++) {
					for (int w = 0; w < dataset.vocabulary.length; w++)
						optTopicWordDist[z][w] = topicWordDist[z][w];
				}
				// set optimized user topical interest, authority and hub
				executor = Executors.newFixedThreadPool(nParallelThreads);
				for (int i = 0; i < nParallelThreads; i++) {
					Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "updateOpt");
					executor.execute(worker);
				}
				executor.shutdown();
				while (!executor.isTerminated()) {
					// do nothing, just wait for the threads to finish
				}
			}
			System.out.printf("likelihood after %d steps: %f, max %f\n", iter, currentLikelihood, maxLikelihood);
		}
		getOptLikelihoodPerplexity_TwitterLDA();
		getOptLikelihoodPerplexity_OriginalLDA();
		getLastLikelihoodPerplexity_TwitterLDA();
		getLastLikelihoodPerplexity_OriginalLDA();

		// print out the learned parameters
		output_OptTopicInterest();
		output_OptAuthority();
		output_OptHub();
		output_OptPostTopicTopWords(50);
		output_OptLikelihoodPerplexityMode0();
		output_OptLikelihoodPerplexityMode1();
		output_LastTopicInterest();
		output_LastAuthority();
		output_LastHub();
		output_LastPostTopicTopWords(20);
		output_LastLikelihoodPerplexityMode0();
		output_LastLikelihoodPerplexityMode1();
		//output_OptTopicWord();
		output_optPostTopic();
		// output_userFollowerHub();
		// output_userFolloweeAuthority();
		// output_userFollowerInterests();
		// output_userFolloweeInterests();
		// output_userNonFollowerHub();
		// output_userNonFolloweeAuthority();
	}

	private double getOptPostLikelihood_TwitterLDA(int u, int n) {
		// Compute likelihood of post number n of user number u
		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			p[z] = Math.log10(dataset.users[u].optTopicalInterests[z]);
			for (int i = 0; i < dataset.users[u].posts[n].nWords; i++) {
				int w = dataset.users[u].posts[n].words[i];
				p[z] += Math.log10(optTopicWordDist[z][w]);
			}
		}
		return MathTool.log10Sum(p);
	}

	private double getOptPostLikelihood_OriginalLDA(int u, int j) {
		// Compute likelihood of post number j of user number u
		double logLikelihood = 0;
		for (int i = 0; i < dataset.users[u].posts[j].nWords; i++) {
			int w = dataset.users[u].posts[j].words[i];
			// Probability that word i is generated by other topics
			double p = 0;
			for (int z = 0; z < nTopics; z++) {
				double p_z = optTopicWordDist[z][w] * dataset.users[u].optTopicalInterests[z];
				p = p + p_z;
			}
			logLikelihood = logLikelihood + Math.log10(p);
		}
		return logLikelihood;
	}

	private double getLastPostLikelihood_TwitterLDA(int u, int j) {
		// Compute likelihood of post number j of user number u
		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			p[z] = Math.log10(dataset.users[u].topicalInterests[z]);
			for (int i = 0; i < dataset.users[u].posts[j].nWords; i++) {
				int w = dataset.users[u].posts[j].words[i];
				p[z] += Math.log10(topicWordDist[z][w]);
			}
		}
		return MathTool.log10Sum(p);
	}

	private double getLastPostLikelihood_OriginalLDA(int u, int j) {
		// Compute likelihood of post number j of user number u
		double logLikelihood = 0;
		for (int i = 0; i < dataset.users[u].posts[j].nWords; i++) {
			int w = dataset.users[u].posts[j].words[i];
			// Probability that word i is generated by other topics
			double p = 0;
			for (int z = 0; z < nTopics; z++) {
				double p_z = topicWordDist[z][w] * dataset.users[u].topicalInterests[z];
				p = p + p_z;
			}
			logLikelihood = logLikelihood + Math.log10(p);
		}
		return logLikelihood;
	}

	private void getOptLikelihoodPerplexity_TwitterLDA() {
		postOptLogLikelidhoodMode0 = 0;
		postOptLogPerplexityMode0 = 0;
		int nTestPost = 0;
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int t = 0; t < dataset.users[u].nPosts; t++) {
				double logLikelihood = getOptPostLikelihood_TwitterLDA(u, t);
				if (dataset.users[u].postBatches[t] == batch)
					postOptLogLikelidhoodMode0 += logLikelihood;
				else {
					postOptLogPerplexityMode0 += (-logLikelihood);
					nTestPost++;
				}
			}
		}
		postOptLogPerplexityMode0 /= nTestPost;
	}

	private void getOptLikelihoodPerplexity_OriginalLDA() {
		postOptLogLikelidhoodMode1 = 0;
		postOptLogPerplexityMode1 = 0;
		int nTestPost = 0;
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int t = 0; t < dataset.users[u].nPosts; t++) {
				double logLikelihood = getOptPostLikelihood_OriginalLDA(u, t);
				if (dataset.users[u].postBatches[t] == batch)
					postOptLogLikelidhoodMode1 += logLikelihood;
				else {
					postOptLogPerplexityMode1 += (-logLikelihood);
					nTestPost++;
				}
			}
		}
		postOptLogPerplexityMode1 /= nTestPost;
	}

	private void getLastLikelihoodPerplexity_TwitterLDA() {
		postLastLogLikelidhoodMode0 = 0;
		postLastLogPerplexityMode0 = 0;
		int nTestPost = 0;
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int t = 0; t < dataset.users[u].nPosts; t++) {
				double logLikelihood = getLastPostLikelihood_TwitterLDA(u, t);
				if (dataset.users[u].postBatches[t] == batch)
					postLastLogLikelidhoodMode0 += logLikelihood;
				else {
					postLastLogPerplexityMode0 += (-logLikelihood);
					nTestPost++;
				}
			}
		}
		postLastLogPerplexityMode0 /= nTestPost;
	}

	private void getLastLikelihoodPerplexity_OriginalLDA() {
		postLastLogLikelidhoodMode1 = 0;
		postLastLogPerplexityMode1 = 0;
		int nTestPost = 0;
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int t = 0; t < dataset.users[u].nPosts; t++) {
				double logLikelihood = getLastPostLikelihood_OriginalLDA(u, t);
				if (dataset.users[u].postBatches[t] == batch)
					postLastLogLikelidhoodMode1 += logLikelihood;
				else {
					postLastLogPerplexityMode1 += (-logLikelihood);
					nTestPost++;
				}
			}
		}
		postLastLogPerplexityMode1 /= nTestPost;
	}

	public void output_OptTopicWord() {
		try {
			File f = new File(outputpath + "/l_OptTopicalWordDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int k = 0; k < nTopics; k++) {
				String text = Integer.toString(k);
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					text = text + "," + Double.toString(optTopicWordDist[k][w]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical word file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_LastTopicWord() {
		try {
			File f = new File(outputpath + "/l_LastTopicalWordDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int k = 0; k < nTopics; k++) {
				String text = Integer.toString(k);
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					text = text + "," + Double.toString(topicWordDist[k][w]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical word file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void output_OptPostTopicTopWords(int k) {
		try {
			File f = new File(outputpath + "/l_OptTopTopicWords.csv");
			BufferedWriter bw = new BufferedWriter(new FileWriter(f.getAbsoluteFile()));
			RankingTool rankTool = new RankingTool();
			WeightedElement[] topWords = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				topWords = rankTool.getTopKbyWeight(dataset.vocabulary, optTopicWordDist[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topWords[j].name + "," + topWords[j].weight + "\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out post topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void output_LastPostTopicTopWords(int k) {
		try {
			File f = new File(outputpath + "/l_LastTopTopicWords.csv");
			BufferedWriter bw = new BufferedWriter(new FileWriter(f.getAbsoluteFile()));
			RankingTool rankTool = new RankingTool();
			WeightedElement[] topWords = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				topWords = rankTool.getTopKbyWeight(dataset.vocabulary, topicWordDist[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topWords[j].name + "," + topWords[j].weight + "\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out post topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_OptTopicInterest() {
		try {
			File f = new File(outputpath + "/l_OptUserTopicalInterestDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optTopicalInterests[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_LastTopicInterest() {
		try {
			File f = new File(outputpath + "/l_LastUserTopicalInterestDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.topicalInterests[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	public void output_GibbTopicInterest() {
		try {
			File f = new File(outputpath + "/l_GibbUserTopicalInterestDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.topicalInterests[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_OptAuthority() {
		try {
			File f = new File(outputpath + "/l_OptUserAuthorityDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				if (currUser.followers != null) {
					text = text + "," + currUser.followers.length;
				} else {
					text = text + ",0";
				}
				if (currUser.followings != null) {
					text = text + "," + currUser.followings.length;
				} else {
					text = text + ",0";
				}
				if (currUser.nonFollowers != null) {
					text = text + "," + currUser.nonFollowers.length;
				} else {
					text = text + ",0";
				}
				if (currUser.nonFollowings != null) {
					text = text + "," + currUser.nonFollowings.length;
				} else {
					text = text + ",0";
				}
				if (currUser.posts != null) {
					text = text + "," + currUser.posts.length;
				} else {
					text = text + ",0";
				}
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optAuthorities[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to authority file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_LastAuthority() {
		try {
			File f = new File(outputpath + "/l_LastUserAuthorityDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.authorities[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to authority file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_OptHub() {
		try {
			File f = new File(outputpath + "/l_OptUserHubDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				if (currUser.followers != null) {
					text = text + "," + currUser.followers.length;
				} else {
					text = text + ",0";
				}
				if (currUser.followings != null) {
					text = text + "," + currUser.followings.length;
				} else {
					text = text + ",0";
				}
				if (currUser.nonFollowers != null) {
					text = text + "," + currUser.nonFollowers.length;
				} else {
					text = text + ",0";
				}
				if (currUser.nonFollowings != null) {
					text = text + "," + currUser.nonFollowings.length;
				} else {
					text = text + ",0";
				}
				if (currUser.posts != null) {
					text = text + "," + currUser.posts.length;
				} else {
					text = text + ",0";
				}
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optHubs[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_LastHub() {
		try {
			File f = new File(outputpath + "/l_LastUserHubDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.hubs[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_OptLikelihoodPerplexityMode0() {
		try {
			File f = new File(outputpath + "/l_OptLikelihoodPerplexityMode0.csv");
			FileWriter fo = new FileWriter(f);
			fo.write("PostLogLikelihood:" + postOptLogLikelidhoodMode0 + "\n");
			fo.write("PostLogPerplexity:" + postOptLogPerplexityMode0 + "\n");
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_OptLikelihoodPerplexityMode1() {
		try {
			File f = new File(outputpath + "/l_OptLikelihoodPerplexityMode1.csv");
			FileWriter fo = new FileWriter(f);
			fo.write("PostLogLikelihood:" + postOptLogLikelidhoodMode1 + "\n");
			fo.write("PostLogPerplexity:" + postOptLogPerplexityMode1 + "\n");
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_LastLikelihoodPerplexityMode0() {
		try {
			File f = new File(outputpath + "/l_LastLikelihoodPerplexityMode0.csv");
			FileWriter fo = new FileWriter(f);
			fo.write("PostLogLikelihood:" + postLastLogLikelidhoodMode0 + "\n");
			fo.write("PostLogPerplexity:" + postLastLogPerplexityMode0 + "\n");
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_LastLikelihoodPerplexityMode1() {
		try {
			File f = new File(outputpath + "/l_LastLikelihoodPerplexityMode1.csv");
			FileWriter fo = new FileWriter(f);
			fo.write("PostLogLikelihood:" + postLastLogLikelidhoodMode1 + "\n");
			fo.write("PostLogPerplexity:" + postLastLogPerplexityMode1 + "\n");
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_optPostTopic() {
		try {
			File f = new File(outputpath + "/l_OptPostTopic.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				for (int p = 0; p < currUser.nPosts; p++) {
					if (currUser.postBatches[p] == batch) {
						String text = currUser.posts[p].postId;
						text = text + "," + Double.toString(currUser.posts[p].topic);
						fo.write(text + "\n");
					}
				}
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to authority file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_userFollowerHub() {
		try {
			File f;
			FileWriter fo;
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String filename = currUser.userId;
				f = new File(outputpath + filename + ".csv");
				fo = new FileWriter(f);
				if (currUser.followers != null) {
					for (int v = 0; v < currUser.nFollowers; v++) {
						int followerId = currUser.followers[v];
						User follower = dataset.users[followerId];
						String text = follower.userId;
						for (int k = 0; k < nTopics; k++) {
							text = text + "," + Double.toString(follower.optHubs[k]);
						}
						fo.write(text + "\n");
					}
				}
				fo.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_userFollowerInterests() {
		try {
			File f;
			FileWriter fo;
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String filename = currUser.userId;
				f = new File(outputpath + filename + ".csv");
				fo = new FileWriter(f);
				if (currUser.followers != null) {
					for (int v = 0; v < currUser.nFollowers; v++) {
						int followerId = currUser.followers[v];
						User follower = dataset.users[followerId];
						String text = follower.userId;
						for (int k = 0; k < nTopics; k++) {
							text = text + "," + Double.toString(follower.optTopicalInterests[k]);
						}
						fo.write(text + "\n");
					}
				}
				fo.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_userFolloweeAuthority() {
		try {
			File f;
			FileWriter fo;
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String filename = currUser.userId;
				f = new File(outputpath + filename + ".csv");
				fo = new FileWriter(f);
				if (currUser.followings != null) {
					for (int v = 0; v < currUser.nFollowings; v++) {
						if (currUser.followingBatches[v] == batch) {
							int followeeId = currUser.followings[v];
							User followee = dataset.users[followeeId];
							String text = followee.userId;
							for (int k = 0; k < nTopics; k++) {
								text = text + "," + Double.toString(followee.optAuthorities[k]);
							}
							fo.write(text + "\n");
						}
					}
				}
				fo.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_userFolloweeInterests() {
		try {
			File f;
			FileWriter fo;
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String filename = currUser.userId;
				f = new File(dataset.path + "/" + mode + "/" + nTopics + "/followee_interests/" + filename + ".csv");
				fo = new FileWriter(f);
				if (currUser.followings != null) {
					for (int v = 0; v < currUser.nFollowings; v++) {
						if (currUser.followingBatches[v] == batch) {
							int followeeId = currUser.followings[v];
							User followee = dataset.users[followeeId];
							String text = followee.userId;
							for (int k = 0; k < nTopics; k++) {
								text = text + "," + Double.toString(followee.optTopicalInterests[k]);
							}
							fo.write(text + "\n");
						}
					}
				}
				fo.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_userNonFollowerHub() {
		try {
			File f;
			FileWriter fo;
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String filename = currUser.userId;
				f = new File(dataset.path + "/" + mode + "/" + nTopics + "/non_follower_hubs/" + filename + ".csv");
				fo = new FileWriter(f);
				if (currUser.nonFollowers != null) {
					for (int v = 0; v < currUser.nNonFollowers; v++) {
						int nonFollowerId = currUser.nonFollowers[v];
						User nonFollower = dataset.users[nonFollowerId];
						String text = nonFollower.userId;
						for (int k = 0; k < nTopics; k++) {
							text = text + "," + Double.toString(nonFollower.optHubs[k]);
						}
						fo.write(text + "\n");
					}
				}
				fo.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_userNonFolloweeAuthority() {
		try {
			File f;
			FileWriter fo;
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String filename = currUser.userId;
				f = new File(
						dataset.path + "/" + mode + "/" + nTopics + "/non_followee_authorities/" + filename + ".csv");
				fo = new FileWriter(f);
				if (currUser.nonFollowings != null) {
					for (int v = 0; v < currUser.nNonFollowings; v++) {
						if (currUser.nonFollowingBatches[v] == batch) {
							int nonFolloweeId = currUser.nonFollowings[v];
							User nonFollowee = dataset.users[nonFolloweeId];
							String text = nonFollowee.userId;
							for (int k = 0; k < nTopics; k++) {
								text = text + "," + Double.toString(nonFollowee.optAuthorities[k]);
							}
							fo.write(text + "\n");
						}
					}
				}
				fo.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	/***
	 * checking if the gradient computation of likelihood by user topical
	 * interest theta_{u,k} is properly implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_TopicalInterest(int u, int k) {
		double DELTA = 1;

		double[] x = dataset.users[u].topicalInterests;

		double f = getLikelihood_topicalInterest(u, x);
		double g = gradLikelihood_topicalInterest(u, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_topicalInterest(u, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(String.format("[TopicInterest] u = %d k = %d DELTA = %.12f numGrad = %f grad = %f\n", u,
					k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;

		}
	}

	/***
	 * checking if the gradient computation of likelihood by A_{v,k} is properly
	 * implemented
	 * 
	 * @param v
	 * @param k
	 */
	public void gradCheck_Authority(int v, int k) {
		double DELTA = 1;

		double[] x = dataset.users[v].authorities;
		double f = getLikelihood_authority(v, x);
		double g = gradLikelihood_authority(v, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_authority(v, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(String.format("[Authority] v= %d k = %d DELTA = %f numGrad = %f grad = %f\n", v, k, DELTA,
					numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;
		}
	}

	/***
	 * checking if the gradient computation of likelihood by H_{u,k} is properly
	 * implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_Hub(int u, int k) {
		double DELTA = 1;

		double[] x = dataset.users[u].hubs;

		double f = getLikelihood_hub(u, x);
		double g = gradLikelihood_hub(u, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_hub(u, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(
					String.format("[Hub] u = %d k = %d DELTA = %f numGrad = %f grad = %f\n", u, k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;

		}
	}

	public void altCheck_TopicalInterest(int u) {
		altOptimize_topicalInterest(u);
	}

	public void altCheck_Authority(int u) {
		altOptimize_Authorities(u);
	}

	public void altCheck_Hub(int u) {
		altOptimize_Hubs(u);
	}

	public static void main(String[] args) {
		double x = 0;
		double a = Math.log(x);
		double b = 1 / x;
		double c = Math.exp(a);
		System.out.printf("a = %f b = %f c = %f\n", a, b, c);
		/*
		 * String datasetPath = "E:/code/java/ctlr/data/synthetic/"; int nTopics
		 * = 10; int batch = 1;
		 * 
		 * larc.ctlr.model.CTLR model = new CTLR(datasetPath, nTopics, batch);
		 * model.init();
		 * 
		 * Random rand = new Random();
		 * 
		 * int u = rand.nextInt(100); int k = rand.nextInt(nTopics);
		 * 
		 * // model.altCheck_TopicalInterest(u); // model.altCheck_Authority(u);
		 * // model.altCheck_Hub(u); model.train(); //
		 * model.gradCheck_Authority(u, k); // model.gradCheck_Hub(u, k); //
		 * model.gradCheck_TopicalInterest(u, k);
		 */
	}
}
